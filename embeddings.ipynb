{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8a7yFUyscIm2"
      },
      "outputs": [],
      "source": [
        "!pip install transformers torch"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Tokenization"
      ],
      "metadata": {
        "id": "MeX0FNLvcykZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "\n",
        "# Load pre-trained BERT model and tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Tokenize input sentence\n",
        "sentence = \"Tokenization is essential for any AI organization.\"\n",
        "tokens = tokenizer.tokenize(sentence)\n"
      ],
      "metadata": {
        "id": "fPpDck_1cMNr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T4EGLPBsdSVs",
        "outputId": "f556514f-fcef-4a40-d248-59dfa46c4c76"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['token', '##ization', 'is', 'essential', 'for', 'any', 'ai', 'organization', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Embeddings\n",
        "\n",
        "Once tokenized, each word (or subword) is mapped to a high-dimensional vector using embeddings. This is where words stop being words and become numbers!\n",
        "\n",
        "Why is this important? Because embeddings capture meaning and relationships between words. Unlike one-hot encoding (where every word is just a unique ID), embeddings represent words in a way that reflects their meaning.\n",
        "\n",
        "For example:\n",
        "ğŸ“ King - Man + Woman â‰ˆ Queen\n",
        "ğŸ“ Paris - France + Italy â‰ˆ Rome\n",
        "\n",
        "This happens because similar words have similar embeddings, and their relationships emerge naturally in the learned space."
      ],
      "metadata": {
        "id": "2swKTF7uc2yc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = tokenizer.encode(sentence, return_tensors=\"pt\")\n",
        "\n",
        "# Get embeddings from BERT\n",
        "with torch.no_grad():\n",
        "    outputs = model(input_ids)\n",
        "    embeddings = outputs.last_hidden_state  # Shape: (1, sequence_length, hidden_size)\n",
        "\n",
        "# Print token-wise embeddings\n",
        "for token, embedding in zip(tokens, embeddings[0]):\n",
        "    print(f\"Token: {token} -> Embedding: {embedding[:5]}...\")  # Showing only first 5 values"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wzSiAkKWdCgo",
        "outputId": "82c456ad-2112-48ac-d118-1094747d9c77"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token: token -> Embedding: tensor([-0.4691, -0.1613, -0.4374,  0.0752, -0.5703])...\n",
            "Token: ##ization -> Embedding: tensor([-0.5442, -0.0559, -0.8279, -0.3151, -0.1229])...\n",
            "Token: is -> Embedding: tensor([-0.5498, -0.5570, -0.5254,  0.3701,  0.4370])...\n",
            "Token: essential -> Embedding: tensor([-0.4953, -0.4014,  0.2123,  0.1217,  0.1697])...\n",
            "Token: for -> Embedding: tensor([ 0.0313,  0.0174, -0.0993,  0.3831,  0.6161])...\n",
            "Token: any -> Embedding: tensor([-0.1266,  0.1941,  0.5250,  0.3075, -0.3373])...\n",
            "Token: ai -> Embedding: tensor([-0.5191,  0.2360,  0.1873, -0.0810, -0.4449])...\n",
            "Token: organization -> Embedding: tensor([-0.3126,  0.3171,  0.2675, -0.0562,  1.0097])...\n",
            "Token: . -> Embedding: tensor([-0.2083, -0.1946, -0.1072, -0.0192,  0.0847])...\n"
          ]
        }
      ]
    }
  ]
}